{"pages":[{"title":"schedule","text":"","link":"/schedule/"}],"posts":[{"title":"System Design","text":"Demystifying Web Infrastructure and Technologies: A Comprehensive GuideIntroductionFrom our everyday interactions with various web applications to the seamless flow of data across platforms, the internet’s vast landscape thrives on numerous technologies working in harmony. As a technology enthusiast, I embarked on a quest to better understand this intricate fabric of interconnected technologies. In this blog, I aim to unravel my findings and offer you a peek into the enigmatic realms of the internet, covering a wide range of topics from the basic to the advanced. The following tutorial video. Vertical ScalingVertical Scaling, also known as “scaling up,” is a strategy that involves augmenting the capacity of a single server, often through the addition of resources like CPU, RAM, or storage. For instance, if a web server struggles with traffic, one might choose to upgrade its processor or increase its memory to improve performance. However, vertical scaling has its limitations: there’s a finite amount of resources you can add to a single server. Horizontal ScalingIn contrast, Horizontal Scaling or “scaling out” involves adding more servers to distribute the load. It’s akin to adding more lanes on a highway to handle increased traffic. Horizontal scaling can provide enhanced redundancy and handle more requests concurrently. Services like Amazon’s Elastic Load Balancer enable automatic horizontal scaling. Load BalancersLoad Balancers play a vital role in both horizontal and vertical scaling. They distribute network traffic across multiple servers to ensure no single server bears an overwhelming load. This distribution enhances application responsiveness, increases availability and reliability, and can dynamically accommodate fluctuating demand. Content Delivery NetworksContent Delivery Networks (CDNs) optimize delivery of web content to users based on their geographical location. CDNs consist of a network of servers distributed globally, serving cached content closer to the user’s location, leading to quicker load times and less bandwidth consumption. CachingCaching is the practice of storing copies of data in high-speed access areas to serve future requests faster. It’s like keeping a snapshot of frequently accessed data so that the system can avoid re-fetching or recomputing it. This process is integral to improving application speed and performance. IP AddressIP Address, short for Internet Protocol Address, is a unique identifier assigned to each device on a network. It facilitates the routing of packets across the internet, similar to how a postal address directs mail to your home. TCP/IPTCP/IP (Transmission Control Protocol/Internet Protocol) is the suite of communication protocols that the internet and most networking architectures use. It facilitates reliable, ordered, and error-checked delivery of data packets between applications running on hosts in an IP network. Domain Name SystemDomain Name System (DNS) translates human-friendly domain names (like www.example.com) into machine-readable IP addresses. It’s akin to a phonebook for the internet, ensuring we don’t have to memorize complex IP addresses. HTTPHTTP (Hypertext Transfer Protocol) is a request-response protocol in client-server computing models. It is the foundation of data communication on the World Wide Web, transmitting hypertext messages between clients and servers. RESTREST (Representational State Transfer) is an architectural style for designing networked applications. It uses HTTP methods (GET, POST, PUT, DELETE) to make calls between machines. It’s stateless and lightweight, making it a popular choice for APIs. GraphQLGraphQL is a query language for APIs, enabling clients to request exactly what they need, reducing over-fetching or under-fetching of data. It empowers the client to dictate the structure of the response, resulting in more efficient network requests. gRPCgRPC (Google Remote Procedure Call) is a high-performance, open-source framework that uses protocol buffers (protobufs) for serializing structured data. gRPC supports multiple programming languages, is highly scalable, and can run in any environment. WebSocketsWebSockets facilitate full-duplex communication channels over a single TCP connection. Unlike HTTP, where the client must initiate communication, WebSockets provide two-way communication between client and server, making them ideal for real-time applications like gaming or chat applications. SQLSQL (Structured Query Language) is a standard language for managing and manipulating relational databases. SQL’s ACID (Atomicity, Consistency, Isolation, Durability) properties ensure reliable processing of database transactions, which is essential for data integrity. NoSQLNoSQL databases emerged to address the limitations of SQL databases, particularly for scaling and for handling unstructured data. They offer flexible schemas and are designed to scale horizontally across servers. ShardingSharding is a type of database partitioning that separates large databases into smaller, faster, more easily managed parts called data shards. Each shard is held on a separate database server instance, spreading the load and reducing the risk of a single point of failure. ReplicationReplication is the process of sharing information across multiple databases to improve reliability, fault-tolerance, or accessibility. It provides a mechanism to ensure data remains consistent across distributed systems. CAP TheoremCAP Theorem is a concept that a distributed computing system cannot simultaneously provide all three of the following guarantees: Consistency (all nodes see the same data), Availability (every request receives a response), and Partition Tolerance (the system continues to operate despite network failures). Message QueuesMessage Queues provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time. They are essential for managing process and thread communication and coordination in complex systems. In summary, the technologies and concepts we’ve covered are fundamental to how the web works, driving our day-to-day interactions online. Understanding these intricacies fosters an appreciation for the complexities that power the digital world and gives us the knowledge to create efficient, reliable systems of our own. I hope that this blog helped demystify these concepts and has sparked your curiosity to dive deeper.","link":"/2023/07/10/20concepts/"},{"title":"Theoretical Basics of Greedy Algorithms","text":"Theoretical Basics of Greedy AlgorithmsBy choosing the local optimum at every stage, we aim to achieve the global optimum. The key to selecting a greedy algorithm is: it’s possible to deduce the global optimum from the local optimum. How to verify if you can use greedy: Provide a counterexample: If you can’t think of a counterexample, try harder. Mathematical induction. Steps for a greedy algorithm: Decompose the problem into several sub-problems. Identify an appropriate greedy strategy. Find the optimal solution for each sub-problem. Stack the local optimums to form a global optimum. Example: Leetcode 455: Distributing Cookies Local Optimum: Give the largest cookie to the child with the biggest appetite. Global Optimum: Feed as many children as possible. 12345678910111213141516171819202122232425class Solution: def sort(self, nums): for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): if nums[j] &lt; nums[i]: temp = nums[i] nums[i] = nums[j] nums[j] = temp def findContentChildren(self, g, s): self.sort(g) self.sort(s) count = 0 i = 0 # pointer to g j = 0 # pointer to s while i &lt; len(g) and j &lt; len(s): if s[j] - g[i] &gt;= 0: count += 1 i += 1 j += 1 else: j += 1 return count Example: Leetcode 376 Swing SequenceThis question is divided into three situations: there are flat slopes in the upper and lower slopes; only the first and last elements; monotonic slopes in which there are flat slopes; (prediff just records the direction of the initial slope when the swing appears)Assume that the rightmost side has a slope, so the initial RESULT = 1.12345678910111213141516171819class Solution: def wiggleMaxLength(self, nums): # only one element if len(nums) == 1: return 1 # three conditions: Up and down with flat slopes; first element; monotone with flat slopes result = 1 prediff = 0 # This is equivalent to adding another element at the beginning of the array with the same value as its first element. curdiff = 0 for i in range(len(nums) - 1): curdiff = nums[i + 1] - nums[i] if (prediff &lt;= 0 and curdiff &gt; 0) or (prediff &gt;= 0 and curdiff &lt; 0): result += 1 # Satisfies the need for a flat slope up and down # Dealing with monotony with flat slopes prediff = curdiff return result Example: Leetcode 53 Maximum Subarray SumIf successive sums are negative, they are immediately discarded and the next number is chosen as the starting position,using greedy algorithm:123456789101112131415class Solution: def maxSubArray(self, nums): result = -1000000 # Set result to a very small value count = 0 # Record the current traversed value for i in range(len(nums)): count += nums[i] if count &gt; result: result = count if count &lt; 0: count = 0 # Reset count to start accumulating from the next position, reflecting the greedy approach return result Example: Leetcode 860 Lemonade To Find ChangeGreedy thought: leave as many 5’s as possible because 5’s are more versatile and can make change not only to 20 but also to 10, whereas 10 can only make change to 20. 12345678910111213141516171819202122232425class Solution: def lemonadeChange(self, bills): five = 0 ten = 0 twenty = 0 for i in range(len(bills)): if bills[i] == 5: five += 1 if bills[i] == 10: if five == 0: return False else: five -= 1 ten += 1 if bills[i] == 20: if ten &gt; 0 and five &gt; 0: ten -= 1 five -= 1 elif five &gt;= 3: five -= 3 else: return False return True","link":"/2023/08/10/Greedy%20Algorithm/"},{"title":"MoreCAP","text":"Diving Deeper into the CAP TheoremAs a fresh graduate exploring the vast landscape of Computer Science, I find myself particularly intrigued by principles that serve as the backbone of the digital world. The CAP Theorem, known as Brewer’s Theorem, is one such principle that provides fundamental guidelines when designing distributed systems. In this blog post, I will try to shed some light on this intricate topic in a comprehensible manner. What is CAP Theorem?The CAP Theorem is a concept in distributed computing that states a distributed data store cannot simultaneously provide all three of the following guarantees: Consistency: Every read receives the most recent write or an error. Availability: Every request receives a response, without guarantee that it contains the most recent write. Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. In simpler terms, when designing distributed systems, we can only guarantee two out of these three properties at any given time. This trade-off has significant implications on the design and usability of distributed systems. Exploring the Trade-offsTo understand these trade-offs better, let’s discuss each of the three guarantees and their implications: Consistency: This means that all nodes in the system see the same data at the same time. Achieving high consistency often requires a sacrifice in availability, especially during network partitions. Highly consistent systems ensure that once a write operation is confirmed, all subsequent read operations will reflect that write. Availability: This means that the system is always ready to process requests and provide responses, irrespective of the state of some individual nodes in the system. Highly available systems ensure that they provide a response to every request, but there’s a chance that some of these responses might not reflect the most recent writes, especially in the case of network partitions. Partition Tolerance: In a distributed system, nodes are often spread across different networks and data centers, which makes network failures inevitable. Partition tolerance means that the system continues to function and uphold its guarantees even when network failures occur. In such cases, depending on whether the system prioritizes consistency or availability, it will either delay responses until the partition is resolved (favouring consistency), or continue to process requests independently on both sides of the partition (favouring availability). Practical Implications of the CAP TheoremThe CAP Theorem guides the design of distributed systems and helps engineers understand the trade-offs that they have to make. For instance: A system like Apache Cassandra opts for Availability and Partition Tolerance (AP), offering eventual consistency where data might not immediately be the same across all nodes but will eventually become consistent. Conversely, a system like Google’s Spanner opts for Consistency and Partition Tolerance (CP), ensuring all nodes have a consistent view of data at the cost of availability during network partitions. It’s essential to remember that these trade-offs depend on the system’s use-case and the application’s specific needs. There’s no one-size-fits-all solution, and system designers often need to make careful considerations about which guarantees to prioritize. In conclusion, the CAP theorem is a fundamental principle in distributed systems design. Although it presents a trade-off situation, understanding these trade-offs allows us to build better, more robust distributed systems. As a new graduate delving deeper into distributed systems, the CAP theorem has been a critical guidepost, illuminating the landscape of design decisions and their implications.","link":"/2023/07/20/MoreCAP/"},{"title":"This is my First Post","text":"Very happy!Today I build up my blog!Main purpose of my blog are Record the my experience of learning computer science. Record my life. I think we can’t study without life since we live in such a beautiful world!Life is not only for study, but also happy, sad, family, friendship and love. I’ll be glad if someone read my blog and get some help.","link":"/2022/01/24/My-New-Post/"},{"title":"System Design","text":"Operating SystemsI’m learning system design and want to post a overview about operating system based on my understanding: File Systems, Virtual Memory, Memory Paging, and Instruction Execution CycleTable of Contents Introduction File Systems Virtual Memory Memory Paging Instruction Execution Cycle Conclusion Introduction An Operating System (OS) is the most crucial program that runs on a computer. It manages the computer’s memory, processes, and all of its software and hardware. It also facilitates communication between the user and the hardware components of the system. Today, we will delve deeper into four key aspects of an OS: File Systems, Virtual Memory, Memory Paging, and the Instruction Execution Cycle. File Systems A file system is a method and data structure that the OS uses to manage files on a disk or partition. It controls how data is stored and retrieved on a disk. Types of file systems include FAT32, NTFS, HFS+, ext4, etc. Each has different features, performance, security, and capacity handling characteristics. I’ve done a File system project in my operating system class. Virtual Memory Virtual memory is a memory management technique used by the OS which gives an application the impression that it has a large, contiguous working memory, while in fact, it might be fragmented and may overflow onto disk storage. It makes the application independent of the physical memory’s actual size. Memory Paging Paging is a memory management scheme implemented by the OS to enable the physical memory’s efficient utilization. It allows the physical address space of a process to be non-contiguous, breaking physical memory into fixed-size blocks called frames and logical memory into blocks of the same size called pages. Instruction Execution Cycle The Instruction Execution Cycle, also known as the fetch-decode-execute cycle, is the basic operational process of a computer. It is the process by which a computer retrieves a program instruction from its memory, determines what actions the instruction requires, and carries out those actions. Conclusion Operating systems are integral to computing. They manage various tasks to allow users to interact seamlessly with their hardware. Understanding the key aspects of an OS such as file systems, virtual memory, memory paging, and the instruction execution cycle can help users and developers leverage the OS for optimized performance.","link":"/2023/07/05/OperatingSystem/"},{"title":"Northwestern University","text":"Today I receive a Master of computer science offer from Northwestern University!!! I’m proud to take the offer and go to this Top10 university in United States.","link":"/2022/10/24/offer/"}],"tags":[{"name":"Life","slug":"Life","link":"/tags/Life/"},{"name":"Computer","slug":"Computer","link":"/tags/Computer/"}],"categories":[{"name":"System Design","slug":"System-Design","link":"/categories/System-Design/"},{"name":"Algorithms","slug":"Algorithms","link":"/categories/Algorithms/"},{"name":"Life","slug":"Life","link":"/categories/Life/"},{"name":"Computer","slug":"Computer","link":"/categories/Computer/"}]}